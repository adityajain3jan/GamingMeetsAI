{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89218de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from mygame import Game2048Env\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=100000):\n",
    "        self.buffer = [None]*max_size\n",
    "        self.max_size = max_size\n",
    "        self.index = 0\n",
    "        self.size = 0\n",
    "    \n",
    "    def insert(self, obj):\n",
    "        self.buffer[self.index] = obj\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "        self.index = (self.index+1)%self.max_size\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        indices = random.sample(range(self.size), batch_size)\n",
    "        return [self.buffer[index] for index in indices]\n",
    "    \n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, obs_shape, num_actions):\n",
    "        super(Model, self).__init__()\n",
    "        assert len(obs_shape)==1 , \"This network only works for flat observations\"\n",
    "        self.obs_shape = obs_shape\n",
    "        self.num_actions = num_actions\n",
    "        #self.net = nn.Sequential(torch.nn.Linear(obs_shape[0], 256), torch.nn.ReLU(), torch.nn.Linear(256, num_actions),)\n",
    "         #torch.nn.ReLU(), nn.Linear(32,32), torch.nn.ReLU(), nn.Linear(32, num_actions),)\n",
    "        self.net = nn.Sequential(torch.nn.Linear(obs_shape[0],512), torch.nn.ReLU(), torch.nn.Linear(512,256), \n",
    "                   torch.nn.ReLU(), torch.nn.Linear(256, 128), torch.nn.ReLU(), torch.nn.Linear(128, num_actions),)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr = 0.0001)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def update_target_model(model, target):\n",
    "    target.load_state_dict(model.state_dict())\n",
    "    print('updating target model')\n",
    "    \n",
    "class Agent:\n",
    "    lr = 1e-4\n",
    "    epsilon_min = 0.05\n",
    "    epsilon_decay_factor = 0.999999\n",
    "    epsilon = 1.0\n",
    "    min_rb_size = 20000\n",
    "    sample_size = 750\n",
    "    env_steps_before_train = 100\n",
    "    tgt_model_update = 1000\n",
    "    device = 'cpu'\n",
    "    env = None\n",
    "    \n",
    "    episode = 0\n",
    "    num_episodes = 1000\n",
    "    step_num = 0\n",
    "    rewards = []\n",
    "    losses = []\n",
    "    average_rewards = []\n",
    "    epsilons = []\n",
    "    num_episodes = []\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rb = ReplayBuffer()\n",
    "        \n",
    "    def start_agent(self, obs_shape, num_actions):\n",
    "        self.model = Model(self.env.observation_space.shape, self.env.action_space.n).to(self.device)\n",
    "        self.target = Model(self.env.observation_space.shape, self.env.action_space.n).to(self.device)\n",
    "        self.update_target_model()\n",
    "        self.obs_shape = obs_shape\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "    def select_action(self, observations, deterministic=False):\n",
    "        self.epsilon_decay()\n",
    "        if np.random.random()>self.epsilon or deterministic:\n",
    "            return self.model(torch.tensor(observations).float()).max(-1)[1].item()\n",
    "        else:\n",
    "            return np.random.randint(0, self.num_actions)\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.rb.insert((state,action,reward,next_state,done))\n",
    "        \n",
    "    def update_target_model(self):\n",
    "        self.target.load_state_dict(self.model.state_dict())\n",
    "        \n",
    "    def train_step(self,state_transitions):\n",
    "        cur_states = torch.stack(([torch.tensor(s[0]) for s in state_transitions])).to(self.device)\n",
    "        actions = torch.stack(([torch.tensor(s[1]) for s in state_transitions]))\n",
    "        rewards = torch.stack(([torch.tensor([s[2]]) for s in state_transitions])).to(self.device)\n",
    "        next_states = torch.stack(([torch.tensor(s[3]) for s in state_transitions])).to(self.device)\n",
    "        mask = torch.stack(([torch.tensor([0]) if s[4] else torch.tensor([1]) for s in state_transitions])).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            qvals_next = self.target(next_states.float()).max(-1)[0]\n",
    "\n",
    "        self.model.optimizer.zero_grad\n",
    "        q_vals = self.model(cur_states.float())\n",
    "        one_hot_actions = F.one_hot(torch.LongTensor(actions), self.num_actions).to(self.device)\n",
    "        loss = ((rewards + mask[:,0]*qvals_next - torch.sum(q_vals*one_hot_actions, -1))**2).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        self.model.optimizer.step()\n",
    "        return loss\n",
    "    \n",
    "    def epsilon_decay(self):\n",
    "        self.epsilon = 1 - (1-self.epsilon_min)*(self.episode/self.num_episodes)\n",
    "        # self.epsilon = max(self.epsilon*self.epsilon_decay_factor, self.epsilon_min)\n",
    "        \n",
    "    def preprocess(self, state, divide=16, logarithmic = False):\n",
    "        if logarithmic:\n",
    "            state = np.log2(1 + state) / divide\n",
    "            return state.reshape(-1)\n",
    "        else:\n",
    "            return state\n",
    "    \n",
    "    def run_episode(self, test = False):\n",
    "        self.episode += 1\n",
    "        self.epsilon_decay()\n",
    "        last_observation = self.preprocess(self.env.reset())\n",
    "        done = False\n",
    "        rolling_reward = 0\n",
    "        \n",
    "        epsilon_initial = self.epsilon\n",
    "        if test:\n",
    "            self.epsilon = 0\n",
    "        \n",
    "        while not done:\n",
    "            self.step_num += 1\n",
    "            action = self.select_action()\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "            observation = self.preprocess(observation)\n",
    "            rolling_reward += reward\n",
    "            \n",
    "            self.rb.insert((last_observation, action, reward, observation, done))\n",
    "\n",
    "        self.epsilon = epsilon_intitial\n",
    "        if test:\n",
    "            return rolling_reward\n",
    "        else:\n",
    "            self.rewards.append(rolling_reward)\n",
    "            rolling_reward = 0\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
